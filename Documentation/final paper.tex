\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx}
\usepackage{longtable}

\citationmode{abbr}
\bibliographystyle{agsm}

\title{Timeframe Trading Algorithms}
\author{A.L. Gillies}
\student{A.L. Gillies}
\supervisor{M. R. Gadouleau}
\degree{MEng Computer Science}

\date{}
\begin{document}
\maketitle

\begin{abstract}
The abstract must be a Structured Abstract with the headings {\bf Context/Background}, {\bf Aims}, {\bf Method}, {\bf Results}, and {\bf Conclusions}.  This section should not be longer than half of a page, and having no more than one or two sentences under each heading is advised.\\

{\bf Context/Background} - Algorithmic trading is characterised by an entirely hands off approach to stock market trading. All data manipulation, mathematical inference, machine learning and trade execution is done autonomously. With this approach, how much of an improvement can be gained over a standard interest rate provided by a high street bank, in the time frame given?\\

{\bf Aims} - Using the average interest rate calculated from British banks, the aim of this paper is to show, through implementation of statistical and machine learning techniques that algorithmic trading can improve the annual return on investment over a given time frame.\\

{\bf Method} - This paper will consider two possibilities for implementation of the system, a purely statistical method, relying on known practices and techniques, and a hybrid system incorporating both statistical reasoning and machine learning. The known statistical practices are mostly used by human traders to allow for data insight and are well vetted. The machine learning techniques are widely used in other contexts, with limited academic papers being available for this area.\\

{\bf Results} - \\

{\bf Conclusions} - \\
\end{abstract}

\begin{keywords}
Algorithmic, Machine Learning, Statistics, R, Trading, Stocks
\end{keywords}

\iffalse
#################################################################################
\fi

\section{Introduction}

\iffalse
This section briefly introduces the general project background, the research question you are addressing, and the project objectives.  It should be between 2 to 3 pages in length.  Do not change the font sizes or line spacing in order to put in more text.

- Same as the aims just longer.\\
- What is the aim for a year?\\
- Objectives.\\
- What is the state of the art in the field?\\
- A bit of the history of trading.\\
\fi

The stock market has been an early adopter of technology since its inception, with companies wanting to get an edge over their fellows and thus earning the most money. The first computer usage in the stock market was in the early 1970s with the New York Stock Exchange introducing the DOT system or the Designated Order Turnaround system, this allowed for bypassing of brokers and routed an order for specific securities to a specialist on the trading floor. Since this point the use of machines to allow for increase throughput and speed has been pandemic. From this point it was inevitable that computers would be used to aid in the decision making process of what to buy or sell and when. This was shown to be very effective and got significant traction in the financial market in 2001 with the showcase of IBMs MGD and  Hewlett-Packard's ZIP, these two algorithmic strategies were shown to consistently outperform their human counterparts. These were both based on academic papers from 1996 so the academic conception of algorithmic applications in financial markets has been present for several decades. Whilst in the current day over one billion shares are traded every day, this would not be possible without computerised assistance. \\

\label{units}
\begin{longtable}{ |p{1.5cm}|p{5.5cm}|p{8cm}| }\hline\hline
Unique ID & Deliverable & Description \\ \hline
DL1 & Simulate the financial market & Have data for at least 10 companies for at least a year, with data for each minute where data is available. \\ \hline
DL2 & Allow buying and selling of stocks & Have a functional buying and selling mechanism, with the data collected for each transaction processed. \\ \hline
DL3 & Implement statistical methods & Implement as many statistical methods as are beneficial to allow for the insight into the data for each stock. \\ \hline
DL4 & Implement a purely statistic strategy & Using just the statistical methods implemented in DL3, create a strategy that will buy and sell stocks to maximise profit made over the time frame given. \\ \hline
DL5 & Create a hybrid strategy & Implement a machine learning trading strategy that uses the stock data as well as any statistical methods that are helpful to maximise profit made over the time frame given. \\ \hline
DL6 & Implement tracking systems & Implement graphical and table outputs for the results of the computer logic and trading performance. \\ \hline
DL7 & Create a testing criteria & Create a method with which to test the strategy so as to avoid over fitting. \\ \hline
\caption{Deliverables}
\end{longtable}

\iffalse
#################################################################################
\fi

\section{Related Work}

\iffalse
This section presents a survey of existing work on the problems that this project addresses.  it should be between 2 to 4 pages in length.  The rest of this section shows the formats of subsections as well as some general formatting information for tables, figures, references and equations.
\fi



\iffalse
#################################################################################
\fi

\section{Solution}

\iffalse
This section presents the solutions to the problems in detail.  The design and implementation details should all be placed in this section.  You may create a number of subsections, each focussing on one issue.
This section should be between 4 to 7 pages in length.
\fi

\iffalse
#################################################################################
\fi

\subsection*{Simulation}

\subsubsection*{Logic}

How the simulation was set up.\\

The logic behind the simulation was to allow trading at a fine grain level, this needed to be permitted by the data chosen to perform the algorithms on. A drawback was that 'tick data' or data for each trade executed was found to be too costly for the project so 'minute data' was used. Minute data provides an open, high, low, and close price for a stock in the given minute, this gave plenty of data points per day of trading. A simplification was created at this point, trading was only done on the price that opened the minute. This was done to reduce the complexity of calculating any inter-minute values. The running of the simulation was based on the progression through each date of available trading and for each day iterate through every minute of the trading day, starting at 09:30 until 16:00. Each trading day has 390 points at which trading is possible, if the data permits it. This is available for X days in the 18 months of data that was used. \\

\subsubsection*{Data}

Talk through what data was used.\\

Forty Five different stocks were used in this paper, taking from each the maximum number of data points available between 01/03/2016 and 01/09/2017 giving an eighteen month window in which the first six months did not allow for trading to occur, only for data collection and training of models, which will be discussed in depth further in the paper, then the final twelve months are for testing each algorithmic approach. There were some limitations with the data that was used. Due to the volatility of the stock market and the fact that some days trading was halted for a myriad of reasons, there are not 390 data points per day, for eighteen months. Trading is not available on weekends, or on bank or national holidays, or if trading is halted for some other reason. This meant that the solution had to take into account holes in the data.\\


Talk through why that data was used.\\

The data was taken from the NASDAQ, the second largest stock market in the world. The number of stocks used was arbitrarily big enough to provide enough data to test any given algorithm, some of which were found to prefer an abundance of data whilst others were much more short sighted. \\

\subsubsection*{Set up}

Why was the simulation set up that way?\\

The simulation was set up in an iterative way in order to minimise the disruptions that missing data would cause, whilst also being exhaustive and searching for data given a reference point is straightforward, so no data point is missed or an inaccurate search is done. A simple testing framework was set up during implementation, variable controlled which stocks were being iterated through at any given moment,what dates were available, when any trade execution could commence, and the amount of capital that was available to any given algorithm. Once these had been established then each date from the start date to the end date variables was found through merged table of all dates across all stocks as some stocks are available to trade when others are not, these are then iterated through. Within this loop is another very similar loop that performs the same action but for minutes rather than dates. All available times are found for the specific date, and iterated through. Finally, for each stock that is available, during testing not all stocks were used so as to help with the removal of bugs, instead a random number generator would choose a predefined number of stocks and iterate over these instead. A stock value is found for that date, at that time, for that stock. Once this has been done then then calculations can be made to decide if any action needs to be taken given this new information. Any buy or sell actions are handled by a single function each that will take in all current information, including the time, date, and stock price. These two functions shall be known as the 'shouldBuy' and 'shouldSell' functions throughout this paper. These functions will be given all available information and will then make a decision using the methods they have been assigned. Any action that is taken will be stored in tables set up before the simulation was started. There are three tables, initially only two were used but a third was introduced at a later date to increase the speed of execution of the simulation. These tables are 'Active', 'Sold', and 'Ledger'. Active is any bundle of stocks that have been bought by the algorithm, stored as a row containing; A unique identifying number to allow for easy retrieval of the row, the date on which the stock was bought, the time at which the stock was bought, the name of the stock, the number of shares that were bought during this execution, and the cost per share or stock price as these are the same. This table is iterated through every time a new stock price becomes available for a stock that is present in the table and the 'shouldSell' function is called, then any necessary calculations are done to decide if this bundle of stocks should be sold or not. If it is decided that this bundle should be sold then this row is removed from the 'Active' table and added to the 'Sold' table. This table contains the same headers as the 'Active' table with the addition of the total amount bought, meaning number of shares multiplied by price they were bought at, the date sold on, the time sold at, the price per share that they were sold at, and the total amount sold. The two totals, total bought and total sold, may seem to be superfluous by they are relied upon by the final table. This table is the most used, it has a row added each time a change is made within the simulation regardless of any actions taken. Each minute a new row is added to this table to keep track of all capital that is within the simulation, the reason for this table is to show at the end of the simulation all trades that were performed and the amount of capital that is liquid or in stocks at any given moment. During implementation minute by minute updates were not required and this only served to increase the execution time of the simulation by a significant amount, and thus the function that adds a new row to this table was only called at the end of each day iteration instead of at the end of each minute iteration. This table contains the heading; date - this was a concatenation of both time and date with a simplified name, value - this was the total value within the simulation, stock value - this is the total amount of capital that are currently within stocks, and capital value - this is the amount of liquid capital available. \\

ADD THREE TABLES TO SHOW HOW THEY LOOK\\

\subsubsection*{Functions}

What functions are critical to the running of the simulation? \\

\iffalse
#################################################################################
\fi

\subsection*{Statistical Methods}

What statistical methods were used first?\\

Once a working simulation had been established and all functions had been shown to be working through hard coding of behaviour, statistical methods had to be implemented. These were separated into two groups, the first are called technical overlays, these are methods that provided insight into the data by providing numbers that are on the same scale as the price itself, whilst technical indicators are the other group and these give insight into the data through numbers that are in no way related to the stock price, and thus allow much more insightful comparisons to be made inter-stock as opposed to intra-stock, that is comparing two stocks is easier if technical indicators are used as opposed to using technical overlays which are very dependant on the stock price and do not translate as well between two unique stocks. \\

\subsubsection*{Basic Functions}

What are some basic functions that need to be defined before they are used?\\



\iffalse
#################################################################################
\fi

\subsubsection*{Technical Overlays}

\textbf{Bollinger Bands}  - Developed by John Bollinger, these are volatility bands placed above and bellow the current stock price and are based on the standard deviation. These are designed to give an indication of how the volatility of a given stock changes over time. For any given stock over a time frame X, the three bands are calculated as such:\\

Upper band $=$ Simple Moving Average$(X)$ $+$ (Standard Deviation$(X) * 2$)\\
Middle band $=$ Simple Moving Average$(X)$\\
Upper band $=$ Simple Moving Average$(X)$ $-$ (Standard Deviation$(X) * 2$)\\

The standard time period is 20 days.\\

\iffalse
[Visual knowledge discovery and machine learning for investment strategy]
[Bollinger on bollinger bands]
\fi

\textbf{Chandelier Exit} - Developed by Charles Le Beau, this is designed to help stay within a trend and not to exit early. In the case of an uptrend the Chandelier Exit will typically be below the stock price and the inverse is true in the case of a downtrend. To calculate it over a time period X:\\

Long = High(X) - (Average True Range(X) * 3)\\
Short = Low(X) + (Average True Range(X) * 3)\\

The standard time period is 22 days.\\

\iffalse
[]
\fi

\textbf{Ichimoku Cloud} - A multifaceted indicator developed by Goichi Hosoda, a Japanese journalist. This is an average based trend identifying indicator based on the standard candlestick charts. This indicator is used as a basis in a number of other theories including Target Price Theory. There are five plots within Ichimoku Cloud.\\

Using time period X.\\
Tenkan-sen or Conversion line = (High(X) + Low(X)) / 2  - Default X is 9 \\
Kijun-sen or Base line = (High(X) + Low(X)) / 2 - Default X is 26 \\
Senkou Span A or Leading span A = (Conversion Line + Base Line) / 2 \\
Senkou Span B or Leading span B = (High(X) + Low(X)) / 2 - Default X is 52 \\
Chikou Span or Lagging span = CloseXPeriodsAgo(X) - Default X is 26\\

\iffalse
[]
\fi

\textbf{Kaufman's Adaptive Moving Average (KAMA)} - Created by Perry Kaufman, this indicator is designed to remove market noise during volatile periods. It takes three parameters, X, Y, and Z. X is the number of periods that is used by the first step of the calculation, known as the efficiency ratio. This will be shown later. The second is the number of periods for the first and fastest exponential moving average or EMA. Third is the number of periods for the second and slowest EMA. The defaults for these values are (10, 2, 30). \\

Efficiency Ratio = Change/Volatility\\
Change = Absolute Value(Close(Now) - CloseXPeriodsAgo(X)) - Default X is 10 \\
Volatility = Sum(Absolute Value(Close - CloseXPeridsAgo(X))) - Default X is 1, this sum is done 10 times, for the last 10 changes in price.\\

The next stage of KAMA is a smoothing constant is calculated. \\

Smoothing Constant = ((Efficiency Ratio x (fastest SC - slowest SC)) + slowest SC)$^2$\\

Final stage is the use of the previous KAMA value to calculate the next value. \\

New KAMA = Previous KAMA + (Smoothing Constant x (Current Price - Previous KAMA))\\

WORK ON THIS ONE \\

\iffalse
[]
\fi

\textbf{Keltner Channels} - Very similar to Bollinger Bands but instead of using standard deviation average true range is used. Created by Chester Keltner, this indicator is made up of three lines in a similar way to Bollinger Bands.\\

Upper Channel Line = Exponential Moving Average(X) + (2 x Average True Range(Y))\\ - Default X = 20, Y = 10 \\
Middle Line = Exponential Moving Average(X)\\ - Default X = 20 \\
Lower Channel Line = Exponential Moving Average(X) - (2 x Average True Range(Y))\\ - Default X = 20, Y = 10 \\

\iffalse
[]
\fi

\textbf{Moving Averages} - These can come in multiple forms with multiple names. The catch all term for this type of smoothing average is a moving average. The most simple is known as a Simple Moving Average. This takes the average of the last X data points. There is no weighting or extra steps. A more complex version is the Exponential Moving Average, this uses weighting to give the more recent values more significance in the calculation. The initial value of EMA is the same as the SMA for the same period as EMA requires an initial value.\\

EMA$_{Today}$ = (Current Stock Price x K) + (EMA$_{Yesterday}$ x (1 - K)) \\
K = 2 / (N + 1) \\
N = Number of periods over which the EMA is applied \\

\iffalse
[]
\fi

\textbf{Moving Average Envelopes} - Based on a Moving Average, this is a percentage based envelope that provide parallel bands above and below the Moving Average. Gives an indication of trends in the data as well as an indicator for stocks that are overbought and oversold when the trend is flat.\\

Upper Envelope = MovingAverage(X) + (MovingAverage(X) x Y)\\
Lower Envelope = MovingAverage(X) - (MovingAverage(X) x Y)\\
Typical values X = 20 and Y = 0.025\\

\iffalse
[]
\fi

\textbf{Parabolic SAR} - Developed by Welles Wilder. SAR stands for 'stop and reverse', this was called a Parabolic Time/Price System. This indicator follows the stock price as the trend is formed, and will then 'stop and reverse' when the trend ends, to follow the new trend. This is one of the more complex indicators.\\

In the case of a rising SAR:

EP or extreme point is a variable that is equal to the highest value of the current uptrend.\\
AF or acceleration factor is a variable that starts at 0.02 and is increment by 0.02 each time the EP is changed, meaning that it is incremented each time a new high is reached. The maximum value of AF is 0.2. \\

SAR = SAR$_{Yesterday}$ + AF$_{Yesterday}$(EP$_{Yesterday}$ - SAR$_{Yesterday}$) \\

In the case of a falling SAR:

This uses the same variable names but inverse behaviour, the EP is equal to the lowest point in the current downtrend. AF is the same but is incremented when EP reaches a new low. \\

SAR = SAR$_{Yesterday}$ - AF$_{Yesterday}$(EP$_{Yesterday}$ - SAR$_{Yesterday}$) \\

These are used to indicate a trend and once a price falls to the other side of the value calculated in the current trend, that trend is over and SAR will flip to the opposite trend. \\

\iffalse
[]
\fi

\textbf{Pivot Points} - An overlay used to indicate directional movement and then shows these in potential support and resistance levels. These are predictive indicators and they exist in multiple forms, the most well known are the standard, Denmark, and Fibonacci versions. These are calculated using the previous days high, low, and close values and are then not recalculated throughout the trading day. A calculation has multiple components, the pivot point, multiple supports, and multiple resistances.\\

Standard Pivot Points. \\
Pivot Point = (High + Low + Close)/3\\
Support One = (PP x 2) - High\\
Support Two = PP - (High - Low)\\
Resistance One = (PP x 2) - Low\\
Resistance Two = PP + (High - Low)\\

Denmark Pivot Points. These are the most complex calculations as they have conditional statements in them and do not use the same calculation methods as the other two.\\
Pivot Point = X / 4\\
Support One = (X / 2) - High\\
Resistance One = (X / 2) - Low\\

Where X is calculated: \\
If Close $<$ Open: X = High + (2 x Low) + Close\\
If Close $>$ Open: X = (2 x High) + Low + Close\\
If Close = Open: X = High + Low + (2 x Close)\\

Fibonacci Pivot Points. \\
Pivot Point = (High + Low + Close)/3\\
Support One = PP - (0.382 x (High  -  Low))\\
Support Two = PP - (0.618 x (High  -  Low))\\
Support Three = PP - (1 x (High  -  Low))\\
Resistance One = PP + (0.382 x (High  -  Low))\\
Resistance Two = PP + (0.618 x (High  -  Low))\\
Resistance Three = PP + (1 x (High  -  Low))\\

\iffalse
[]
\fi

\textbf{Price Channels} - Using three calculations to show an upper, lower, and middle bound, used to indicate the start of an upwards or downward trend and act accordingly.\\

Upper = High(X)\\
Center = (High(X) + Low(X)) / 2\\
Lower = Low(X)\\
The default X value is 20.\\

\iffalse
[]
\fi

\iffalse
#################################################################################
\fi

\subsubsection*{Technical Indicators}

\textbf{Aroon} - Developed by Tushar Chande, Aroon is indicative of the strength of the current trend. It was designed to be similar but uniquely different to standard momentum oscillators, which focus on price relative to time, whilst Aroon focuses on time relative to price. It has two components, Up and Down, both are shown as percentages. Up will maximise on an upward trend and Down will maximise on a downward trend.\\

Aroon Up = ((X - DaysSinceHigh(X))/X) x 100\\
Aroon Down = ((X - DaysSinceLow(X))/X) x 100\\
Default value of X = 25. //

\iffalse
[]
\fi

\textbf{Aroon Oscillator} - A join of the two values of Aroon into a single value.\\

Aroon Oscillator = Aroon Up - Aroon Down\\

\iffalse
[]
\fi

\textbf{Average Directional Index (ADX)} - This is part of a group of indicators developed by Welles Wilder. The group is made up of Average Directional Index, Minus Directional Index, and Plus Directional Index, and is called the Directional Movement System. \\

COMPLEX NEED TIME \\

\iffalse
[]
\fi

\textbf{Average True Range (ATR)} - Developed by J. Welles Wilder as a measure for volatility, ATR has been used in a wide variety of applications outside of the financial world. The initial idea was based around a concept called True Range, calculated as such:\\

The greatest of:\\
High(X) - Low(X)\\
ABS(High(X) - PreviousClose)\\
ABS(Low(X) - PreviousClose)\\

This was then used in conjunction with the previous ATR to calculate the new ATR.\\

New ATR = ((Prev ATR x (X-1)) + TR) / X - Default X is 14 \\

\iffalse
[]
\fi

\textbf{BandWidth}  - One of the two indicators derived from Bollinger Bands by John Bollinger, the other being \%B. This is a single value that takes all Bollinger Bands as components.\\

Bandwidth = ((Upper Band - Lower Band) / Middle Band) x 100 \\

\iffalse
[]
\fi

\textbf{\%B Indicator} - Another Bollinger Band derivative, \%B indicator gives an indication as to the relationship of the current price and the Upper and Lower Bollinger Bands. \\

\%B = (Current Price - Lower Band) / (Upper Band - Lower Band)\\

\iffalse
[]
\fi

\textbf{Chande Trend Meter (CTM)} - Developed by Tushar Chande. This indicator assigns a numerical score to a stock based on several other indicators.\\

NEED TO RESEARCH CANT FIND AN EQUATION\\

\iffalse
[]
\fi

\textbf{Commodity Channel Index (CCI)} - Developed by Donald Lambert. Used to show a comparison between the current price and the average price over a given timespan. Uses multiple other calculations as component parts, including Simple Moving Average, Typical Price, and Mean Deviation.\\

Typical Price = (High + Low + Close) / 3\\

Mean Deviation = SUM(ABS(Period Value - Period Average)) / X\\
Default X = 20. Find the sum of the deviation from the average value of the last 20 periods within each period. \\

CGI = (Typical Price - X period SMA of Typical Price) / (0.05 x Mean Deviation)//
Default X = 20.\\

\iffalse
[]
\fi

\textbf{Coppock Curve} - Developed by Edwin Coppock. Using a Weighted Moving Average as well as a period based Rate Of Change, this simple indicator as been used by many as a sell indicator as the value crosses the positive-negative boundary. It is calculated as the WMA of the ROC plus the ROC over a different period.\\

Coppock Curve = WeightedMovingAverage(X, RateOfChange(Y)) + RateOfChange(Z)\\
Default X = 10, Y = 14, Z = 11. \\

\iffalse
[]
\fi

\textbf{Correlation Coefficient} - This is a measure of the similarities between two stocks, this is useful to identify trends that effect many stocks. This uses the variance of two stocks using an average price over a given time frame as well as the covariance between them.\\

Variance(Stock$_1$) = Average(Price$^2$, X) - (Average(Price, X)$^2$)\\
Variance(Stock$_2$) = Average(Price$^2$, X) - (Average(Price, X)$^2$)\\
Covariance(Stock$_1$, Stock$_2$) = Average(Price(Stock$_1$) x Price(Stock$_2$), X) - (Average(Price(Stock$_1$), X) x Average(Price(Stock$_2$), X))\\
Correlation Coefficient = Covariance / SQRT(Variance(Stock$_1$) x Variance(Stock$_2$))\\

\iffalse
[]
\fi

\textbf{DecisionPoint Price Momentum Oscillator (PMO)} - \\

\iffalse
[]
\fi

\textbf{Detrended Price Oscillator (DPO)} \\

\iffalse
[]
\fi

\textbf{Ease of Movement (EMV)} \\

\iffalse
[]
\fi

\textbf{Force Index} \\

\iffalse
[]
\fi

\textbf{Mass Index} \\

\iffalse
[]
\fi

\textbf{MACD (Moving Average Convergence/Divergence Oscillator)} \\

\iffalse
[]
\fi

\textbf{MACD Histogram} \\

\iffalse
[]
\fi

\textbf{Money Flow Index (MFI)} \\

\iffalse
[]
\fi

\textbf{Negative Volume Index (NVI)} \\

\iffalse
[]
\fi

\textbf{On Balance Volume (OBV)} \\

\iffalse
[]
\fi

\textbf{Percentage Price Oscillator (PPO)} \\

\iffalse
[]
\fi

\textbf{Percentage Volume Oscillator (PVO)} \\

\iffalse
[]
\fi

\textbf{Price Relative / Relative Strength} \\

\iffalse
[]
\fi

\textbf{Pring's Know Sure Thing (KST)} \\

\iffalse
[]
\fi

\textbf{Pring's Special K} \\

\iffalse
[]
\fi

\textbf{Rate of Change (ROC) and Momentum} \\

\iffalse
[]
\fi

\textbf{Relative Strength Index (RSI)} \\

\iffalse
[]
\fi

\textbf{RRG Relative Strength} \\

\iffalse
[]
\fi

\textbf{StockCharts Technical Rank (SCTR)} \\

\iffalse
[]
\fi

\textbf{Slope} \\

\iffalse
[]
\fi

\textbf{Standard Deviation (Volatility)} \\

\iffalse
[]
\fi

\textbf{Stochastic Oscillator (Fast, Slow, and Full)} \\

\iffalse
[]
\fi

\textbf{StochRSI} \\

\iffalse
[]
\fi

\textbf{TRIX} \\

\iffalse
[]
\fi

\textbf{True Strength Index} \\

\iffalse
[]
\fi

\textbf{Ulcer Index} \\

\iffalse
[]
\fi

\textbf{Ultimate Oscillator} \\

\iffalse
[]
\fi

\textbf{Vortex Indicator} \\

\iffalse
[]
\fi

\textbf{Williams \%R} \\

\iffalse
[]
\fi

\iffalse
#################################################################################
\fi

\subsection*{Machine Learning}



\iffalse
#################################################################################
\fi

\section{Results}

\iffalse
this section presents the results of the solutions.  It should include information on experimental settings.  The results should demonstrate the claimed benefits/disadvantages of the proposed solutions.
This section should be between 2 to 3 pages in length.
\fi



\iffalse
#################################################################################
\fi

\section{Evaluation}

\iffalse
This section should between 1 to 2 pages in length.
\fi



\iffalse
#################################################################################
\fi

\section{Conclusions}

\iffalse
This section summarises the main points of this paper.  Do not replicate the abstract as the conclusion.  A conclusion might elaborate on the importance of the work or suggest applications and extensions.  This section should be no more than 1 page in length.
\fi



\iffalse
#################################################################################
\fi

\iffalse
\section*{------------------------------------------------------------}

\subsection{Main Text}

The font used for the main text should be Times New Roman (Times) and the font size should be 12.  The first line of all paragraphs should be indented by 0.25in, except for the first paragraph of each section, subsection, subsubsection etc. (the paragraph immediately after the header) where no indentation is needed.

\subsection{Figures and Tables}
In general, figures and tables should not appear before they are cited.  Place figure captions below the figures; place table titles above the tables.  If your figure has two parts, for example, include the labels ``(a)'' and ``(b)'' as part of the artwork.  Please verify that figures and tables you mention in the text actually exist.  make sure that all tables and figures are numbered as shown in Table \ref{units} and Figure 1.
%sort out your own preferred means of inserting figures

\begin{table}[htb]
\centering
\caption{UNITS FOR MAGNETIC PROPERTIES}
\vspace*{6pt}
\label{units}
\begin{tabular}{ccc}\hline\hline
Symbol & Quantity & Conversion from Gaussian \\ \hline
\end{tabular}
\end{table}

\subsection{References}

The list of cited references should appear at the end of the report, ordered alphabetically by the surnames of the first authors.  References cited in the main text should use Harvard (author, date) format.  When citing a section in a book, please give the relevant page numbers, as in \cite[p293]{budgen}.  When citing, where there are either one or two authors, use the names, but if there are more than two, give the first one and use ``et al.'' as in  , except where this would be ambiguous, in which case use all author names.

You need to give all authors' names in each reference.  Do not use ``et al.'' unless there are more than five authors.  Papers that have not been published should be cited as ``unpublished'' \cite{euther}.  Papers that have been submitted or accepted for publication should be cited as ``submitted for publication'' as in \cite{futher} .  You can also cite using just the year when the author's name appears in the text, as in ``but according to Futher \citeyear{futher}, we \dots''.  Where an authors has more than one publication in a year, add `a', `b' etc. after the year.

\iffalse
#################################################################################
\fi

\begin{table}[htb]
\centering
\caption{SUMMARY OF PAGE LENGTHS FOR SECTIONS}
\vspace*{6pt}
\label{summary}
\begin{tabular}{|ll|c|} \hline
& \multicolumn{1}{c|}{\bf Section} & {\bf Number of Pages} \\ \hline
I. & Introduction & 2--3 \\ \hline
II. & Related Work & 2--3 \\ \hline
III. & Solution & 4--7 \\ \hline
IV. & Results & 2--3 \\ \hline
V. & Evaluation & 1-2 \\ \hline
VI. & Conclusions & 1 \\ \hline
\end{tabular}
\end{table}

\iffalse
#################################################################################
\fi

\section*{Notes for the final paper}

\subsection*{Data}
- taken from a website
- between dates
- in format
- using R and shell scripts managed
- put into a single file
- read in
- initially had an issue with data, not enough
- was rectified and the same steps were followed to put it in the same format
- dumbing of the data was done ? might not mention this but we only take the open price of each minute, thus reducing the dataload, this would not be done in a full scale system as we would have all the metrics and data from the past and all maths that would be used has already been calculated.

\subsection*{Notes}
- ghomas has suggested that we run the code on the GPU as we are running a rolling average
- It could be run on hamilton
- This will take some research but we will continue in the research department first
- Then we will do this parallelism
- then we will implement some of the researched techniques

- the functions we need to implement are:
- when to buy
- when to sell
- how much to buy

- a buy and hold strategy could be implemented first, this would be a buy early in the year and hold until the end, any short term fluctuations would not have any effect but the exact time to buy and sell could be tricky, this in an initial buy at a low price and a late sell at a high price, any fluctuations in the middle are not important only the difference in almost the whole market as this is a conservative approach to trading and will spread the bet.

- genetic trading algorithm, this is more of a technique to make a strategy rather than a strategy itself, the idea is to 'learn' how to trade, using a fitness function and other such techniques on a strategy that is already in place in order to fine tune it. This may be hard to implement but could have some merit.

- there is the possibility of making some machine learning algorithms that will do the same thing, this will have to be researched in R as this is new to me. The techniques are not, but the language in conjunction is.

- some hands on trading strategies could be considered, just with the algorithm being the active party not the person. These include position trading, swing trading, scalping, and day trading. These can be considered.

- alpha is something that may be useful, it is a metric used to measure risk. Alpha is one of five technical risk ratios; the others are beta, standard deviation, R-squared, and the Sharpe ratio. All these will be looked into, to see if any benefit can be drawn from them. They may be useful in later calculations.
\fi

\end{document}